# Tutorial: Tutorial Chat Llama Nemotron

This project is a chat application powered by an **NVIDIA LLM (Large Language Model)**. It allows users to chat with an AI that can access and utilize a *knowledge base* to provide more informed responses. The application uses a **RAG (Retrieval-Augmented Generation) system** to enhance the LLM's knowledge.


**Source Repository:** [https://github.com/NVIDIA/GenerativeAIExamples/tree/main/community/chat-llama-nemotron](https://github.com/NVIDIA/GenerativeAIExamples/tree/main/community/chat-llama-nemotron)

```mermaid
flowchart TD
    A0["NVIDIA Dynamo Backend
"]
    A1["LLM Proxy
"]
    A2["RAG Service
"]
    A3["RAG Backend Application (FastAPI)
"]
    A4["Configuration Loader
"]
    A5["React Frontend Components
"]
    A6["Frontend App Configuration
"]
    A0 -- "Provides LLM" --> A1
    A1 -- "Serves LLM" --> A5
    A2 -- "Managed by" --> A3
    A3 -- "Exposes API" --> A5
    A4 -- "Provides config" --> A3
    A4 -- "Provides config" --> A6
    A5 -- "Uses config" --> A6
    A5 -- "Requests LLM" --> A1
    A5 -- "Upload/Search RAG" --> A3
    A2 -- "Augments prompts" --> A0
```

## Chapters

1. [React Frontend Components
](01_react_frontend_components_.md)
2. [Frontend App Configuration
](02_frontend_app_configuration_.md)
3. [LLM Proxy
](03_llm_proxy_.md)
4. [RAG Service
](04_rag_service_.md)
5. [RAG Backend Application (FastAPI)
](05_rag_backend_application__fastapi__.md)
6. [NVIDIA Dynamo Backend
](06_nvidia_dynamo_backend_.md)
7. [Configuration Loader
](07_configuration_loader_.md)


---

Generated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)